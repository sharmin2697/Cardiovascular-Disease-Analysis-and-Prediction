---
title: ""
author: "Mishkin Khunger, Sanat Lal, Sharmin Kantharia, Vishal Pathak"
date: "11/8/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r dataload}
library(ggplot2)


cd = read.csv("D:\\Data Science\\Cardio Vascular\\Cardio-Vascular\\cardio.csv")

dim(cd)

cd$age<- (cd$age/365)

cd$age<- round(cd$age) 
```

```{r basicchecks}

#Structure
str(cd)

#Summary
summary(cd)

# Anomalies found in height, weight, systolic (ap_hi) and diastolic (ap_lo)
#Gender,cholestrol,gluc,smoke,alco,active,cardio are needed to be converted into categorical variable

```
```{r lib}
library(pastecs)
library(psych)
library(Hmisc)
```

```{r Outlierdetectionofage}
fivenum(cd$age)
psych::describe(cd$age)
stat.desc(cd$age)
hist(cd$age,
     main = "Histogram of Age",
     xlab = "Age in Years")
boxplot(cd$age,
        main = toupper("Boxplot of Age"),
        ylab = "Age in years",
        col = "blue")
outlier_values <- boxplot.stats(cd$age)$out  # outlier values.
boxplot(cd$age, main="Age", boxwex=0.1)
mtext(paste("Outliers: ", paste(outlier_values, collapse=", ")), cex=0.6)

```
As we can see skewness in -0.3 which means age is approximately symmetric. As if skewness is in between scale of (-0.5 to 0.5)it will be considered approximately symmetric.


#### so in age as we can see the outliers are value 30 which are outside of 1.5*IQR . We are going to use capping methiod in order to treat outlier. For missing values that lie outside the 1.5*IQR limits, we could cap it by replacing those observations outside the lower limit with the value of 5th %ile and those that lie above the upper limit, with the value of 95th %ile.

```{r Outliertreatmentofage}
qnt <- quantile(cd$age, probs=c(.25, .75), na.rm = T)
caps <- quantile(cd$age, probs=c(.05, .95), na.rm = T)
H <- 1.5 * IQR(cd$age, na.rm = T)
cd$age[cd$age < (qnt[1] - H)] <- caps[1]
cd$age[cd$age > (qnt[2] + H)] <- caps[2]

outlier_values <- boxplot.stats(cd$age)$out  # outlier values.
boxplot(cd$age, main="Age", boxwex=0.1)
mtext(paste("Outliers: ", paste(outlier_values, collapse=", ")), cex=0.6)

``` 
#### As we can see now the age column has no outliers.we are good to go.

```{r HeightEDA}
fivenum(cd$height)
psych::describe(cd$height)
stat.desc(cd$height)

hist(cd$height,
     main = "Histogram of Height",
     xlab = "Height in centimeter")

outlier_values <- boxplot.stats(cd$cd$height)$out  # outlier values.
boxplot(cd$height, main="Height", boxwex=0.1)
mtext(paste("Outliers: ", paste(outlier_values, collapse=", ")), cex=0.6)

outliersper <- function(x){
  length(which(x >  mean(x) + 3 * sd(x) | x < mean(x) - 3 * sd(x))  ) / length(x)
}

print(outliersper(cd$height))

```
#### As we can see there are few of outliers in Height variable , which is around .004 percent of total data. since this is continous data so we are going treat with capping method. For missing values that lie outside the 1.5*IQR limits, we could cap it by replacing those observations outside the lower limit with the value of 5th %ile and those that lie above the upper limit, with the value of 95th %ile. Moreover the skewness is around -0.63 which means height column is left skewed.

```{r Heightoutlierimpute}

qnt <- quantile(cd$height, probs=c(.25, .75), na.rm = T)
caps <- quantile(cd$height, probs=c(.05, .95), na.rm = T)
H <- 1.5 * IQR(cd$height, na.rm = T)
cd$height[cd$height < (qnt[1] - H)] <- caps[1]
cd$height[cd$height > (qnt[2] + H)] <- caps[2]

outlier_values <- boxplot.stats(cd$height)$out  # outlier values.
boxplot(cd$height, main="Height", boxwex=0.1)
mtext(paste("Outliers: ", paste(outlier_values, collapse=", ")), cex=0.6)

```
#### As we can see now there are no outliers in height column.Also  earlier there was skewness in the column and it was (-0.63) but after the outlier treatment it reduces to 0.09 . so height variable is not skewed.so we are good to go.


```{r WeightEDA}
fivenum(cd$weight)
psych::describe(cd$weight)
stat.desc(cd$weight)

hist(cd$weight,
     main = "Histogram of Weight",
     xlab = "Weight in kg")

outlier_values <- boxplot.stats(cd$weight)$out  # outlier values.
boxplot(cd$weight, main="Weight", boxwex=0.1)
mtext(paste("Outliers: ", paste(outlier_values, collapse=", ")), cex=0.6)

outliersper <- function(x){
  length(which(x >  mean(x) + 3 * sd(x) | x < mean(x) - 3 * sd(x))  ) / length(x)
}

print(outliersper(cd$weight))

```
#### As we can see there are 10 % of data which are outlier . Also data is highly asymmetric as skew value is (1.01) which meamns age is highly right skewed.we will first treat outlier ,then we will check if skewness persists then we will apply transformation on the weight column.



```{r Weightoutlierimpute}

qnt <- quantile(cd$weight, probs=c(.25, .75), na.rm = T)
caps <- quantile(cd$weight, probs=c(.05, .95), na.rm = T)
H <- 1.5 * IQR(cd$weight, na.rm = T)
cd$weight[cd$weight < (qnt[1] - H)] <- caps[1]
cd$weight[cd$weight > (qnt[2] + H)] <- caps[2]

outlier_values <- boxplot.stats(cd$height)$out  # outlier values.
boxplot(cd$height, main="Height", boxwex=0.1)
mtext(paste("Outliers: ", paste(outlier_values, collapse=", ")), cex=0.6)

psych::describe(cd$weight)

```
#### As we can see now there are no outliers and also skewness is 0.39 which means our data is approximately normal.we are good to go.

```{r ap_hi(EDA)}
fivenum(cd$ap_hi)
psych::describe(cd$ap_hi)
stat.desc(cd$ap_hi)

hist(cd$ap_hi,
     main = "Histogram of systolic Blood Pressure",
     xlab = "Ap_hi")

outlier_values <- boxplot.stats(cd$ap_hi)$out  # outlier values.
boxplot(cd$ap_hi, main="systolic Blood Pressure", boxwex=0.1)
mtext(paste("Outliers: ", paste(outlier_values, collapse=", ")), cex=0.6)

outliersper <- function(x){
  length(which(x >  mean(x) + 3 * sd(x) | x < mean(x) - 3 * sd(x))  ) / length(x)
}

print(outliersper(cd$ap_hi))

```
#### As we can see there is a 8 units of difference in mean and Meadian.which is really high . But the outliers are only around .05 % . This means that the values of outliers are of very values. Aslo the skew and kurtosis (85.29,7579.32)values are really very high. which again point to the fact that some values are incorrect or garbage values . we are to going treat the outliers in the next section and then we will analyse our column again.

```{r SBPoutlierimpute}

qnt <- quantile(cd$ap_hi, probs=c(.25, .75), na.rm = T)
caps <- quantile(cd$ap_hi, probs=c(.05, .95), na.rm = T)
H <- 1.5 * IQR(cd$ap_hi, na.rm = T)
cd$ap_hi[cd$ap_hi < (qnt[1] - H)] <- caps[1]
cd$ap_hi[cd$ap_hi > (qnt[2] + H)] <- caps[2]

outlier_values <- boxplot.stats(cd$ap_hi)$out  # outlier values.
boxplot(cd$ap_hi, main="systolic Blood Pressure", boxwex=0.1)
mtext(paste("Outliers: ", paste(outlier_values, collapse=", ")), cex=0.6)

psych::describe(cd$ap_hi)

```

#### yes we were absolutely right the outliers were of very high and they were garbage value.After treating outliers , we can see our sweet boxplot(without any outliers) .Moreover our skew value is now under approximately normal range which is 0.54.

```{r ap_low(EDA)}
fivenum(cd$ap_lo)
psych::describe(cd$ap_lo)
stat.desc(cd$ap_lo)

hist(cd$ap_lo,
     main = "Histogram of Diastolic Blood Pressure",
     xlab = "Ap_low")

outlier_values <- boxplot.stats(cd$ap_lo)$out  # outlier values.
boxplot(cd$ap_lo, main="Diastolic Blood Pressure", boxwex=0.1)
mtext(paste("Outliers: ", paste(outlier_values, collapse=", ")), cex=0.6)

outliersper <- function(x){
  length(which(x >  mean(x) + 3 * sd(x) | x < mean(x) - 3 * sd(x))  ) / length(x)
}

print(outliersper(cd$ap_lo))

```

#### As we can see there is difference of 16 units in mean and median.From the histogram we can see that data is right skewed which is around (32.11). Also the kurtosisis really high. there are around 10% outliers so we are going to treat it.after that we are going to check again

```{r DBDoutliertreatment}

qnt <- quantile(cd$ap_lo, probs=c(.25, .75), na.rm = T)
caps <- quantile(cd$ap_lo, probs=c(.05, .95), na.rm = T)
H <- 1.5 * IQR(cd$ap_lo, na.rm = T)
cd$ap_lo[cd$ap_lo < (qnt[1] - H)] <- caps[1]
cd$ap_lo[cd$ap_lo > (qnt[2] + H)] <- caps[2]

outlier_values <- boxplot.stats(cd$ap_lo)$out  # outlier values.
boxplot(cd$ap_lo, main="Diastolic Blood Pressure", boxwex=0.1)
mtext(paste("Outliers: ", paste(outlier_values, collapse=", ")), cex=0.6)

psych::describe(cd$ap_lo)

```
#### As we can see now after treatment of outliers , skew has also been in expected range(0.48) . now our variable is approximately normally distributed.

#Missing Value check and treatment on continous variables
```{r Missingvalues}

sum(is.na(cd$age))
sum(is.na(cd$height))
sum(is.na(cd$weight))
sum(is.na(cd$ap_hi))
sum(is.na(cd$ap_lo))

```

```{r genderEDA}
cd[cd$gender==1,]$gender<-"F"
cd[cd$gender==2,]$gender<-"M"
cd$gender<-as.factor(cd$gender)
table(factor(cd$gender))
```

#### There are no missing variables in the data , which means that we are good to go for our bivariate analysis.



```{r Checkaphivsaplo}

temp_min = pmin(cd$ap_hi, cd$ap_lo)
cd$ap_hi = pmax(cd$ap_hi, cd$ap_lo)
cd$ap_lo = temp_min
```
#### since diastolic and systolic are connected and diastolic will always be greater than systolic . so for sanity we have implement few changes to swap values if sysytolic is less than diastolic


```{r calculateBMI}

cd$bmi <- cd$weight/(cd$height*cd$height)*10000


```

```{r fac, include=FALSE}
#Gender
cd$gender<-as.factor(cd$gender)
#Cholestrol
cd$cholesterol<-as.factor(cd$cholesterol)
#Glucose
cd$gluc<-as.factor(cd$gluc)
#Smoke
cd$smoke<- as.factor(cd$smoke)
#Alcohol
cd$alco<- as.factor(cd$alco)
#Active
cd$active<- as.factor(cd$active)
#Cardio
cd$cardio<- as.numeric(cd$cardio)
```

```{r subsettingforfactor}
library(GPArotation)
l1 <- data.frame(cd$age ,cd$height,cd$weight , cd$ap_hi,cd$ap_lo , cd$bmi)
corrm<- cor(l1)     
```

```{r screeplot}
scree(corrm, factors=TRUE, pc=TRUE, main="scree plot", hline=NULL, add=FALSE) ### SCREE PLOT
eigen(corrm)$values                                                     ### EIGEN VALUES

```


#### As we can see the scree plot there are only 3 values which have eigen value above 1 , this means that we are going to use factor analysis for PC3.

```{r cumvarvar}
library(plyr)
eigen_values <- mutate(data.frame(eigen(corrm)$values)
                       ,cum_sum_eigen=cumsum(eigen.corrm..values)
                       , pct_var=eigen.corrm..values/sum(eigen.corrm..values)
                       , cum_pct_var=cum_sum_eigen/sum(eigen.corrm..values))
```

```{r factor analysis}
library(psych)
FA<-fa(r=corrm, 3, rotate="varimax", SMC=FALSE, fm="minres")
print(FA)                                                  
FA_SORT<-fa.sort(FA)                                        
ls(FA_SORT)                                                 
FA_SORT$loadings
```

#### From factor analysis we have taken 3 factors in account as per factor loadings. and those are bmi , api_hi , height. So we are good to go for our model implementation.




```{r cardioEDA}
cd[cd$cardio==0,]$cardio<-"Negative"
cd[cd$cardio==1,]$cardio<-"Positive"
cd$cardio<-as.factor(cd$cardio)
table(factor(cd$cardio))
```



#### There are no missing variables in the data , which means that we are good to go for our bivariate analysis.



## Calculating BMI

```{r bmi, echo=FALSE}
cd$bmi<- cd$weight/(cd$height*cd$height)*10000 # Weight in kgs and height in cms

```
## Dropping the id column
```{r drop id, echo=FALSE}
cd$id<- NULL


```

```{r levels}
levels(cd$gender)
levels(cd$cholesterol)
levels(cd$gluc)
levels(cd$smoke)
levels(cd$alco)
levels(cd$active)

```

```{r dummy variables}
cd$cholnormal  <- ifelse(cd$cholesterol == 1, 1, 0)
cd$cholabovenorm <- ifelse(cd$cholesterol == 2, 1, 0)
sum(cd$cholabovenorm)
sum(cd$cholnormal)

cd$glucnorm <- ifelse(cd$gluc == 1, 1, 0)
cd$glucabovenorm <- ifelse(cd$gluc == 2, 1, 0)
sum(cd$glucnorm)
sum(cd$glucabovenorm)

```

#### so we have create 2 dummy variables for cholestrol and 2 dummy variaables for glucose category and in both the columns there are 3 levels

```{r visualization1}
library(ggplot2)

#age count , affected vs not affected
counts <- table(cd$cardio, cd$age)
barplot(counts, main="Age distribution",
  xlab="Age", ylab="Counts", col=c("darkblue","red"),
  legend = rownames(counts), beside=TRUE)


```


#### We see that most people who are suffering from cardio vascular diseases are of the age 60, followed by 56.
Majorly, people belonging to the age group 50+ are suffering from the disease.



```{r v2}


#plot(cd$age,cd$ap_hi, main="Scatterplot Example",
 #  xlab="Car Weight ", ylab="Miles Per Gallon ")

library(car)
scatterplot(cd$ap_hi ~ cd$age, data=cd,
   xlab="Age", ylab="Systolic Blood pressure",
   main="Relationship of age and systolic blood pressure",
   )


mytable <- table(cd$cholesterol )
lbls <- paste(names(mytable), "\n", mytable, sep="")
pie(mytable, labels = lbls,
   main="Pie Chart of Cholestrol levels")

```


#### There are three cholestrol levels-
1- Normal
2- Above Normal
3- Way above normal

The above pie chart shows that 3/4th of the population come under the normal cholestrol category.We see that most people who are suffering from cardio vascular diseases are of the age 60, followed by 56.
Majorly, people belonging to the age group 50+ are suffering from the disease.


```{r v3}
library("ggplot2")

    ggplot(cd, aes(x= cholesterol,  group=cd$gender)) + 
    geom_bar(aes(y = ..prop.., fill = factor(..x..)), stat="count") +
    geom_text(aes( label = scales::percent(..prop..),
                   y= ..prop.. ), stat= "count", vjust = -.5) +
    labs(y = "Percent", fill="cholestrol") +
    facet_grid(~gender) +
    scale_y_continuous(labels = scales::percent)

```

#### The above graph shows that there are comparitively higher chances of women to have a cardio vascular disease where 76.9% of the women have normal cholestrol level.


```{r v4}

library("ggplot2")
attach(cd)
plot(gender, cardio, main="Scatterplot Example",
   xlab="gender ", ylab="Cardio ", pch=19)

counts <- table(cd$cardio, cd$gender)
barplot(counts, main="Gender distribution",
  xlab="Gender", ylab="Counts", col=c("pink","red"),
  legend = rownames(counts), beside=TRUE)

```

#### The above graph suggests that there are higher number of cases for women, but there are higher number of negative cases for women and higher number of positive cases for men.


```{r glvscd, include=FALSE, echo=TRUE}
  
  library("ggplot2")
  attach(cd)
  
  counts <- table(cd$cardio, cd$gluc)
  barplot(counts, main="Glucose distribution",
    xlab="Glucose", ylab="Counts", col=c("blue","black"),
    legend = rownames(counts), beside=TRUE)

```


```{r chcd, include=FALSE,echo=TRUE}
  
  library("ggplot2")
  attach(cd)

  
  counts <- table(cd$cardio, cd$cholesterol)
  barplot(counts, main="Cholestrol vs Cardio",
    xlab="Cholesterol", ylab="Counts", col=c("black","red"),
    legend = rownames(counts), beside=TRUE)
```
  
```{r dividedata, include=FALSE}
require(caTools) #install caTools
set.seed(123)
sample = sample.split(cd,SplitRatio = 0.70)
train_cd =subset(cd,sample ==TRUE)
test_cd =subset(cd, sample==FALSE)
```

#### we are dividing the data into test and train.


```{r stat, echo = TRUE}

# Relationship with the target variable


ttestresage = t.test(train_cd$age~train_cd$cardio)
ttestresage
```
####Strong significance

#### Statistical Significance between Height & Cardio Vascular Disease risk 
```{r hvscd}
ttestresheight = t.test(train_cd$height~train_cd$cardio)
ttestresheight
```
#### Comparitively weak significance

#### Statistical Significance between Weight & Cardio Vascular Disease risk 
```{r wvscd}
ttestresweight = t.test(train_cd$weight~train_cd$cardio)
ttestresweight
```
#### Strong significance

```{r bvscd, include=FALSE}
#Statistical Significance between BMI & Cardio Vascular Disease risk 
ttestresbmi = t.test(train_cd$bmi~train_cd$cardio)
ttestresbmi
```
#### Strong significance

#### Statistical Significance between Systolic & Cardio Vascular Disease 
```{r svdcd, include=FALSE}
ttestressys = t.test(train_cd$ap_hi~train_cd$cardio)
ttestressys 
```
#### Strong significance

#### Statistical Significance between Diastolic & Cardio Vascular Disease 
```{r dvscd, include=FALSE}
ttestresdia = t.test(train_cd$ap_lo~train_cd$cardio)
ttestresdia 
```
#### Strong significance



#### Statistical Significance between Cholestrol & Cardio Vascular Disease
```{r cvdcd, include=FALSE}
chisqrescholesterol = chisq.test(train_cd$cholesterol,train_cd$cardio)
chisqrescholesterol 
```
#### Strong significance

#### Statistical Significance between Glucose & Cardio Vascular Disease
```{r gvscd, include=FALSE}
chisqresgluc = chisq.test(train_cd$gluc,train_cd$cardio)
chisqresgluc 
```
#### Strong significance

#### Statistical Significance between Glucose & Cardio Vascular Disease
```{r avscd, include=FALSE}
chisqresalco = chisq.test(train_cd$alco,train_cd$cardio)
chisqresalco 
```
#### Not statistically significant but missing by a small margin

#### Statistical Significance between Physical Activity & Cardio Vascular Disease risk 
```{r scvscd, include=FALSE}
chisqresact = chisq.test(train_cd$active,train_cd$cardio)
chisqresact
```
#### Strong significance


```{r logisticmodel, include=FALSE, echo=TRUE}

fit <- glm(cardio ~ bmi+weight+ap_hi+height+glucnorm+glucabovenorm+smoke+alco+active+cholnormal+cholabovenorm , data = train_cd , family = binomial(logit) )
summary(fit)
```
#### As you can see the factors we have used to build model is giving results . For most of the variables the p value is low that means they are effective in predicting the cario deseas e. bmi and height are 2 variables which are not significant . alos the diffrence in value of null deviance and residual deviance is high . which means model is fitting.

```{r letscheck, include=FALSE, echo=TRUE}

require(MASS)
step3<- stepAIC(fit,direction="both")
?stepAIC()
ls(step3)
step3$anova

```   


#### As we can see the step aic functio has removed bmi , from the model and gave our best predictors.Now we gonna run our model again by removing BMI and compare it as well.


```{r secondmodel, include=FALSE, echo=TRUE}
fit2<-glm(cardio ~ weight + ap_hi + height + glucnorm + glucabovenorm + 
    smoke + alco + active + cholnormal + cholabovenorm , data = train_cd , family = binomial(logit) )
summary(fit2)
```
#### As we can see there AIC value for 1st and second model is almost same which means BMI was not impacting the model.

```{r letscheckhl, include=FALSE, echo=TRUE}
library(ResourceSelection)
hl <- hoslem.test(train_cd$cardio, fitted(fit2), g = 10)

cbind(hl$expected , hl$observed)

```


```{r letscheckconcordance, include=FALSE, echo=TRUE}
Concordance = function(GLM.binomial) {
  outcome_and_fitted_col = cbind(GLM.binomial$y, GLM.binomial$fitted.values)
  # get a subset of outcomes where the event actually happened
  ones = outcome_and_fitted_col[outcome_and_fitted_col[,1] == 1,]
  # get a subset of outcomes where the event didn't actually happen
  zeros = outcome_and_fitted_col[outcome_and_fitted_col[,1] == 0,]
  # Equate the length of the event and non-event tables
  if (length(ones[,1])>length(zeros[,1])) {ones = ones[1:length(zeros[,1]),]}
  else {zeros = zeros[1:length(ones[,1]),]}
  # Following will be c(ones_outcome, ones_fitted, zeros_outcome, zeros_fitted)
  ones_and_zeros = data.frame(ones, zeros)
  # initiate columns to store concordant, discordant, and tie pair evaluations
  conc <- rep(NA, length(ones_and_zeros[,1]))
  disc <- rep(NA, length(ones_and_zeros[,1]))
  ties = rep(NA, length(ones_and_zeros[,1]))
  for (i in 1:length(ones_and_zeros[,1])) {
    # This tests for concordance
    if (ones_and_zeros[i,2] > ones_and_zeros[i,4])
    {conc[i] = 1
    disc[i] = 0
    ties[i] = 0}
    # This tests for a tie
    else if (ones_and_zeros[i,2] == ones_and_zeros[i,4])
    {
      conc[i] = 0
      disc[i] = 0
      ties[i] = 1
    }
    # This should catch discordant pairs.
    else if (ones_and_zeros[i,2] < ones_and_zeros[i,4])
    {
      conc[i] = 0
      disc[i] = 1
      ties[i] = 0
    }
  }
  # Here we save the various rates
  conc_rate = mean(conc, na.rm=TRUE)
  disc_rate = mean(disc, na.rm=TRUE)
  tie_rate = mean(ties, na.rm=TRUE)
  Somers_D<-conc_rate - disc_rate
  gamma<- (conc_rate - disc_rate)/(conc_rate + disc_rate)
  #k_tau_a<-2*(sum(conc)-sum(disc))/(N*(N-1)
  return(list(concordance=conc_rate, num_concordant=sum(conc), discordance=disc_rate, num_discordant=sum(disc), tie_rate=tie_rate,num_tied=sum(ties),
              somers_D=Somers_D, Gamma=gamma))
  
}
Concordance(fit2) 
require(car)
vif(fit2)

  
```

#### adj-R square would be totally irrelevant in case of logistic regression because we model the log odds ratio and it becomes very difficult in terms of explain ability.This is where concordance steps in to help. Concordance tells us the association between actual values and the values fitted by the model in percentage terms. Concordance is defined as the ratio of number of pairs where the 1 had a higher model score than the model score of zero to the total number of 1-0 pairs possible. A higher value for concordance (60-70%) means a better fitted model. However, a very large value for concordance (85-95%) could also suggest that the model is over-fitted and needs to be re-aligned to explain the entire population. so as we can see our model have 77 % of concordance whcih means its a good fit.

#### Moving over somers d if you see it is around 0.55 . Somers’ D is an index that you want to be closer to 1 and farther from −1. So we can say that our model is good fitted .

#### Now we are going to see the AUC And KS statsics for which we will need ROCR Package.

```{r Predict, include=FALSE, echo=TRUE }

pred <- predict(fit2 , newdata = test_cd,type = 'response')
y_pred_num <- ifelse(pred > 0.5, 1, 0)
y_pred <- factor(y_pred_num, levels=c(0, 1))
y_act <- test_cd$cardio


#mean(y_pred == y_act)
```
#### As we can see in prediction we have an accuracy rate of 72 %

```{r completely, include=FALSE, echo=TRUE}
library(sqldf)
t1<-cbind(train_cd, Prob=predict(fit2,train_cd , type="response" ))
names(t1)

mean(t1$Prob)
View(t1)

t2<-cbind(test_cd,  Prob=predict(fit2,test_cd, type="response"))  

mean(t2$Prob)
View(t2)

#Decile Analysis Reports - t1(training)

# find the decile locations 
decLocations <- quantile(t1$Prob, probs = seq(0.1,0.9,by=0.1))

# use findInterval with -Inf and Inf as upper and lower bounds
t1$decile <- findInterval(t1$Prob,c(-Inf,decLocations, Inf))

t1_DA <-sqldf("select decile, count(decile) as count, min(Prob) as Min_prob
                     , max(Prob) as max_prob 
              , sum(cardio) as default_cnt
              from t1
              group by decile
              order by decile desc")

View(t1_DA) 
write.csv(t1_DA,"mydata1_DA.csv")


#Decile Analysis Reports - t2(testing)

# find the decile locations 
decLocations <- quantile(t2$Prob, probs = seq(0.1,0.9,by=0.1))

# use findInterval with -Inf and Inf as upper and lower bounds
t2$decile <- findInterval(t2$Prob,c(-Inf,decLocations, Inf))


t2_DA <- sqldf("select decile, count(decile) as count, min(Prob) as Min_prob
                     , max(Prob) as max_prob 
                    , sum(cardio) as default_cnt
                    from t2
                    group by decile
                    order by decile desc")
View(t2_DA)
```




```{r cdch}
cd$cardio<-as.factor(cd$cardio)
```

# Decision Tree

### We use the packages 'rpart' and 'rpart.plot' inorder to compute and plot the decision trees. The rpart() uses our target feature (cardio) against other features. 

#### In the first tree, we compare the target against all the other features in the dataset. Using printcp() and plotcp() we can see the most important features we want to work with. As we can see, our most important deciding feature is ap_hi (systolic blood pressure). 

#### Using cross-validation, we can see the most important variables using complexity parameters. The optimized tree shows us that only one deciding feature is the basis to deicide whether a person has a CVD or not. However, we can also see that the oroginal tree has 5 most important features: ap_hi, ap_lo, cholestrol, age and bmi.

#### We use the packages 'rattle', 'rpart.plot' and 'RColorBrewer', to create a more clean tree with the different probablilities of the possiblilities of Yes and No of attaining CVD.

#### To check for accuracy, use the predict() and use the accuracy formula.

#### The accuracy of this decision tree is 71.66%


```{r DT, echo=FALSE}
#install.packages("party")
#install.packages("rpart")
library("rpart")
library("rpart.plot")

# Creating the tree
dt <- rpart(cardio ~ ., method = "class", data = train_cd) 
rpart.plot(dt, extra = 106)

# Plotting the tree
plot(dt)
text(dt, pretty = 0)

# Required libraries
#install.packages("rattle")
library("rattle")
library("rpart.plot")
library("RColorBrewer")

fancyRpartPlot(dt)

# Cross Validation 
printcp(dt)
plotcp(dt)
summary(dt)

# Accuracy

predict_dt <-predict(dt, test_cd, type = 'class')

table_mat <- table(test_cd$cardio, predict_dt)
table_mat

accuracy_dt <- sum(diag(table_mat)) / sum(table_mat)
accuracy_dt
```




```{r random forest}
library(randomForest)
set.seed(123)
rf_cd <- randomForest(cardio~.-weight-height, data=train_cd,
                   ntree = 500,
                   mtry = 3,
                   importance = TRUE)
print(rf_cd)
attributes(rf_cd)

```
#### Taking Age,Gender, Blood Pressure (Systolic and Diastolic), Alcohol,Smoking,Glucose,Active and BMI as predictors we get an Out of Bag ( set of boostrap datasets which does not contain  certain records from the original dataset) error rate of 26.68%


```{r oober, }
oob.error.data <- data.frame(
  Trees=rep(1:nrow(rf_cd$err.rate), times=3),
  Type=rep(c("OOB", "Negative", "Positive"), each=nrow(rf_cd$err.rate)),
  Error=c(rf_cd$err.rate[,"OOB"], 
    rf_cd$err.rate[,"Negative"], 
    rf_cd$err.rate[,"Positive"]))

ggplot(data=oob.error.data, aes(x=Trees, y=Error)) +
  geom_line(aes(color=Type))
```
#### Here we can see that error rate is the most stable around 500 trees for both the results as well as OOB. Therefore, we will continue with the  default value.

```{r oob, include=FALSE, echo=TRUE}
oob.rf_cd <- vector(length=10)
for(i in 1:10) {
  temp.model <- randomForest(cardio ~ .-weight-height, data=train_cd, mtry=i, ntree=500)
  oob.rf_cd[i] <- temp.model$err.rate[nrow(temp.model$err.rate),1]
}
oob.rf_cd
```

#### Here we can see that, we get the best result for mtry= 3 in terms of Out of Bag error rate

```{r pred1}
library(caret)
p1 <- predict(rf_cd, train_cd)
confusionMatrix(p1, train_cd$cardio)
```
#### The training set gives an accuracy of 76.86% 

```{r pred2}
library(caret)
p2 <- predict(rf_cd, test_cd)
confusionMatrix(p2, test_cd$cardio)
```

#### The test set gives an accuracy of 73.58%
```{r varimpo}
hist(treesize(rf_cd),
     main = "No. of Nodes for the Trees",
     col = "green")

# Variable Importance
varImpPlot(rf_cd,
           sort = T,
           n.var = 10,
           main = "Top 10 - Variable Importance")
importance(rf_cd)
varUsed(rf_cd)

```
#### The variable importance plot shows that in terms of Mean Decrease Gini value we get Systolic(Ap_hi), Diastolic(Ap_lo),bm, age and cholesterol to be our top 5 predictors


```{r KNN}
#install package class
library("class")

head(cd, 20)
table(cd$cardio)

#random number generation
set.seed(9850)
gp <- runif(nrow(cd))
cd <- cd[order(gp),]


head(cd)

str(cd)

#normalizing the numerical features so that none of the features have undue influence on the prediction of our classes

normalize <- function(x) {
  + return ( (x- min(x)) / (max(x) - min(x)))}


cd_n <- as.data.frame(lapply(cd[,c(1,3,4,5,6,13)], normalize))

str(cd_n)

summary(cd_n)

k= sqrt(nrow(cd))

k   #265 , k is the number of neighbours and is computed by taking the suared root of the total number of observations. Preferably should be an odd number beacuse in default mode KNN takes majority vote and in case of a tie its good to have an odd number

m11 <- knn(train = train_cd, test = test_cd, cl=train_cd_target , k=265)

m11


#confusion matrix

table(test_cd_target, m1)

acc <- 100 * sum(test_cd_target == m1)/ NROW(test_cd_target)

acc

#install package caret
library(caret)

confusionMatrix(table(m1,test_cd_target))

#to improve the performance of the model we can use this loop, which will give the best value of k

i=1
k.optm=1
for (i in 1:265){
  
  knn.mod <- knn(train = train_cd, test=test_cd, cl=train_cd_target, k=i)
  
  k.optm[i] <- 100* sum(test_cd_target == knn.mod)/NROW(test_cd_target)
  
  k=i
  
  cat(k,'=', k.optm[i], '\n')
  
}
 
plot(k.optm, type="b", xlab="k-value", ylab="accuracy level")


```

#### For our prediction using K- Nearest Neighbour, the value of k (which is the number of neighbours) is usually chosen taking the squared root of the total number of observations, preferably should be an odd number beacuse in default mode KNN takes majority vote and in case of a tie its good to have an odd number. Hence, for our prediction we are taking k=265.


```{r traink}
m2 <- knn(train = train_cd, test = test_cd, cl=train_cd_target , k=10)

table(test_cd_target, m2)

acc2 <- 100 * sum(test_cd_target == m2)/ NROW(test_cd_target)

acc2

```


```{r Normalization}
#install package class
library("class")

head(cd, 20)
table(cd$cardio)

#random number generation
set.seed(9850)
gp <- runif(nrow(cd))
cd <- cd[order(gp),]


head(cd)

str(cd)


normalize <- function(x) {
  + return ( (x- min(x)) / (max(x) - min(x)))}


cd_n <- as.data.frame(lapply(cd[,c(1,3,4,5,6,13)], normalize))
cd_n_target <- cd[,12]
str(cd_n)

summary(cd_n)

```
#### We normalized the numerical features so that none of the features have undue influence on the prediction of our classes.


```{r KNNt}

#train test split

require(caTools) #install caTools
set.seed(123)
sample = sample.split(cd,SplitRatio = 0.70)
train_cd =subset(cd,sample ==TRUE)
test_cd =subset(cd, sample==FALSE)

train_cd_target <- train_cd[,12]
test_cd_target <- test_cd[,12]

k= sqrt(nrow(cd))
k   

m1 <- knn(train = train_cd, test = test_cd, cl=train_cd_target , k=265)

m1


#confusion matrix


table(test_cd_target, m1)

acc <- 100 * sum(test_cd_target == m1)/ NROW(test_cd_target)

acc



#install package caret
library(caret)

confusionMatrix(table(m1,test_cd_target))

```

#### For our prediction using K- Nearest Neighbour, the value of k (which is the number of neighbours) is usually chosen taking the squared root of the total number of observations, preferably should be an odd number beacuse in default mode KNN takes majority vote and in case of a tie its good to have an odd number. Hence, for our prediction we are taking k=265.

#### Using k as the squared root of the total number of data-points, we are getting the accuracy of 73% which is great but we can try different values of k in order to achieve better accuracy. Further, the Kappa statistic which is a metric that compares the observed accuracy with the expected accuracy(random chance) is comparatively low with a value of 0.4741. Hence we will try different values of k to achieve better 


```{r loopk}
   

i=1
k.optm=1
for (i in 1:10){  knn.mod <- knn(train = train_cd, test=test_cd, cl=train_cd_target, k=i)
  
  k.optm[i] <- 100* sum(test_cd_target == knn.mod)/NROW(test_cd_target)
  
  k=i
  
  cat(k,'=', k.optm[i], '\n')
  plot(k.optm, type="b", xlab="k-value", ylab="accuracy level")}

```

#### We applied a loop to compute accuracy for k ranging from 2 to 10.


```{r knn2}
m2 <- knn(train = train_cd, test = test_cd, cl=train_cd_target , k=7)

table(test_cd_target, m2)

acc2 <- 100 * sum(test_cd_target == m2)/ NROW(test_cd_target)

acc2

nrow(m2)


```

####  On applying the loop, we attained highest accuracy of 77.35 at k=7. 

### VISUALISATION
